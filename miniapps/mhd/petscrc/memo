run:
exMHD -r 2 -o 2 -tf 3 -vs 100 -dt .002
exMHD -r 2 -tf 10 -vs 50 -dt .004 -i 2

Parallel AMR run:
mpirun -n 16 exAMRMHDp -m xperiodicR1.mesh -rs 0 -rp 2 -o 4 -tf 8 -vs 200 -dt .0001 -i 3 -amrl 3 -ltol 2e-3 -derefine

parallel run:
mpirun -n 4 exMHDp -rs 4 -o 2 -tf 3 -vs 100 -dt .001
mpirun -n 4 exMHDp -rs 4 -tf 10 -vs 200 -dt .001 -i 2
mpirun -n 4 exMHDp -m xperiodicR3.mesh -rs 0 -o 4 -tf 8 -vs 100 -dt .001 -i 3

case 3 example run
mpirun -n 8 exMHDp -m Meshes/xperiodic.mesh -rs 3 -o 4 -tf 8 -vs 100 -dt .001 -i 3
mpirun -n 8 exAMRMHDp -m Meshes/xperiodicR1.mesh -rs 0 -rp 2 -o 4 -tf 8 -vs 200 -dt .0001 -i 3 -amrl 3 -ltol 2e-3 -derefine
srun -n 72 imMHDp -m ./Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf .2 -dt .1 -usepetsc --petscopts ./petscrc/rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4 -im_supg 1 -no-supg -i_supgpre 3 -no-fd -local -lr 3


serial AMR quick test:
./exAMRMHD -m xperiodicR3.mesh -r 0 -o 4 -tf .025 -vs 10 -dt .0001 -i 3 -amrl 3 -ltol 1e-3

Parallel AMR quick test:
./exAMRMHDp -m xperiodicR3.mesh -rs 0 -o 4 -tf .025 -vs 10 -dt .0001 -i 3 -amrl 3 -ltol 1e-3

plot:
glvis -m refined.mesh -g current.sol -k "mmcRjl" -ww 800 -wh 800
glvis -m refined.mesh -g psiPer.sol -k "mmc" -ww 800 -wh 800
glvis -m refined.mesh -g phi.sol -k "mmc" -ww 800 -wh 800
glvis -m refined.mesh -g omega.sol -k "mmc" -ww 800 -wh 800
glvis -m refined.mesh -g psiPer.sol -k "mmRjl" -ww 1000 -wh 1000

parallel plot:
glvis -np 4 -m mesh -g sol_omega -k "mmc" -ww 800 -wh 800
glvis -np 36 -m mesh -g sol_omega -k "mmc" -ww 800 -wh 800
glvis -np 48 -m mesh -g sol_omega -k "mmc" -ww 800 -wh 800
glvis -np 72 -m mesh -g sol_omega -k "mmc" -ww 800 -wh 800
glvis -np 512 -m mesh -g sol_omega -k "mmc" -ww 800 -wh 800
glvis -np 4096 -m mesh -g sol_omega -k "mmc" -ww 1000 -wh 1000

8/20
something wrong with amrl?? is that effective?

8/19
this works well but need to make derefine more aggresively:
srun -n 256 imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 2 -tf 6. -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-4 -derefine -resi 1e-4 -visc 1e-4 -refs 4 -im_supg 1 -no-supg -i_supgpre 3 -no-fd -yrange -err-ratio 0.1 -derefine-ratio 0.5 -err-fraction 0.2 > log-amr6-2.out

this only have a small impact:
srun -n 256 imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 2 -tf 6. -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-4 -derefine -resi 1e-4 -visc 1e-4 -refs 4 -im_supg 1 -no-supg -i_supgpre 3 -no-fd -yrange -err-ratio 0.1 -derefine-ratio 1 -err-fraction 0.2 > log-amr6-2.out

this is better:
srun -n 256 imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 2 -tf 6. -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-2 -derefine -resi 1e-4 -visc 1e-4 -refs 4 -im_supg 1 -no-supg -i_supgpre 3 -no-fd -yrange -err-ratio 0.1 -derefine-ratio .5 -err-fraction 0.2 > log-amr6-4.out

this is the best now:
srun -n 256 imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 2 -tf 8. -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-2 -derefine -resi 1e-4 -visc 1e-4 -refs 4 -im_supg 1 -no-supg -i_supgpre 3 -no-fd -yrange -err-ratio 0.1 -derefine-ratio .5 -err-fraction 0.2 > log-amr6-6.out

this is also good:
srun -n 256 imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -tf 8. -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 4 -ltol 1e-2 -derefine -resi 1e-4 -visc 1e-4 -refs 4 -im_supg 1 -no-supg -i_supgpre 3 -no-fd -yrange -err-ratio 0.1 -derefine-ratio .5 -err-fraction 0.1

8/18
amr for 1e-5

rs=4 lr=5 is necessary for 1e-5
srun -N 15 -n 540  --ntasks-per-node=36 ./imMHDp -m ./Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 8. -dt .05 -usepetsc --petscopts rc_debug4 -s 3 -shell -resi 1e-5 -visc 1e-5 -im_supg 1 -supg -i_supgpre 3 -no-fd -local -lr 5 -visit

this is the base line:
mpirun -n 6 imMHDp -m ./Meshes/xperiodic-new.mesh -rs 3 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf .2 -dt .05 -usepetsc --petscopts petscrc/rc_debug4 -s 3 -shell -resi 1e-5 -visc 1e-5 -im_supg 1 -supg -i_supgpre 3 -no-fd -local -lr 3 

mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 10 -im_supg 1 -supg -i_supgpre 3 -no-fd -yrange

with supg the preconditioner seems not working with AMR

0. add supg options into it
1. first reproduce old result
2. fix the initial local refinement (todo)
3. change refinement criterion (maybe use a new way to do refinement?)

smaller ltol is needed for psi and phi estimator (it works):
srun -n 144 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-8 -derefine -resi 1e-4 -visc 1e-4 -refs 4 -im_supg 1 -supg -i_supgpre 3 -no-fd -yrange


larger ltol is needed for psi and j estimator (it works):
srun -n 256 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 2 -tf 3 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-4 -derefine -resi 1e-4 -visc 1e-4 -refs 4 -im_supg 1 -supg -i_supgpre 3 -no-fd -yrange

this may be okay:
imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-9 -derefine -resi 1e-4 -visc 1e-4 -refs 4 -im_supg 1 -supg -i_supgpre 3 -no-fd -yrange --err-ratio 0.5

vs
imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-9 -derefine -resi 1e-4 -visc 1e-4 -refs 4 -im_supg 1 -supg -i_supgpre 3 -no-fd -yrange --err-ratio 0.5 -err-fraction 0.01

this is better:
imAMRMHDp -m ./Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-4 -derefine -resi 1e-4 -visc 1e-4 -refs 4 -im_supg 1 -supg -i_supgpre 3 -no-fd -yrange -err-ratio 0.1 -derefine-ratio 0.5 -err-fraction 0.05

it is doing something reasonable but not put mesh at the location we really want to

8/13
too many technique issues:
1. simulation takes too long (3600+16hours and it find the stiff spot, nersc can run longer but it takes forever to schedule)
2. try to get restart working, so far it can load but a bug found to build the operators

move back and redo the simulation with more cores
maybe a good time to check amr

srun -n 4 ./imMHDp -m ./Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf .05 -dt .05 -usepetsc --petscopts ./petscrc/rc_full -s 3 -shell -resi 1e-3 -visc 1e-3 -no-supg -no-fd -local -lr 2

srun -n 4 restart -m ./Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf .1 -dt .05 -usepetsc --petscopts ./petscrc/rc_full -s 3 -shell -resi 1e-3 -visc 1e-3 -no-supg -no-fd -local -lr 2 -t0 0.05


8/10
after merging to master, imMHDp seems okay

8/5
res=1e-6 starts behave okay (run to t=6 so far, working on later time)
>>> time adapativity works okay but it may introduce oscillations
    1/dt in the diffusion with a floor (or growth rate)
>>> underresolve?
>>> plasmoid happens later time?
>>> working on restart

p refinement: Schur complement grows but # of preconditioner grow quickly (indicates we need to reduce time step)

* better scaling (the algorithm seems really slow)
  still scaled up to 4096
  challenging to run longer time

* better preconditioner
  playing with all parameters in preconditioner: the effective one is err=0.1; 1e-7 starts to work but need more tests

number of plasmoids??

8/4
todo:
amr
p refinement

test:
srun -n 256 ./imMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf .2 -dt .05 -usepetsc --petscopts ./rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6 -im_supg 1 -supg -i_supgpre 3 -no-fd -local -lr 4

srun -N 32 -n 1024  --ntasks-per-node=32 ./imMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 2 -tf .2 -dt .025 -usepetsc --petscopts ./rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6 -im_supg 1 -supg -i_supgpre 3 -no-fd -local -lr 5
1024 interactive: 831s on nersc
2048 interactive: 400s on nersc

srun -N 64 -n 2048  --ntasks-per-node=32 ./imMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 2 -tf .2 -dt .025 -usepetsc --petscopts ./rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6 -im_supg 1 -supg -i_supgpre 3 -no-fd -local -lr 5

7/30
improved the preconditioner (commute or not)
1e-5 works well with dt=.05; amr probably will also work
1e-6 still running
discuss HDG results: the stoppage criterion? reduce time step?

better rc_debug for res=1e-6 (let it solve more accurately in each nonlinear solver)
rtol=2e-4
rtol0=0.1
for instance rtol0=.3 will fail at a very earlier time

cfl estimate?
flow speed and B 
time stepping? adaptive (yes)

AMR for 1e-5?

the reduce time step gives strange solution? The diffusion seems not enough

determine the possible ideal number of cores
let lr=5 be the resolution to get res=1e-6 (h=5e-4)

7/23
1. improve the preconditioner 
done. which version is better? check res=1e-5 and res=1e-6
2. improve the Schur complement solver (ParaSails works better, Euclid behaves strangely)
3. test the case for 1e-6 

srun -n 72 ./imMHDp -m ./Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf .2 -dt .05 -usepetsc --petscopts ./petscrc/rc_simple -s 3 -shell -resi 1e-7 -visc 1e-7 -im_supg 1 -supg -i_supgpre 3 -no-fd -local -lr 4

srun -n 72 ./imMHDpNew -m ./Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf .1 -dt .05 -usepetsc --petscopts ./petscrc/rc_debug4 -s 3 -shell -resi 1e-6 -visc 1e-6 -im_supg 1 -supg -i_supgpre 3 -fd -local -lr 6 

on badger debug:
srun -n 2 ./imMHDp -m ./Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -dt .05 -usepetsc --petscopts ./petscrc/rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 -im_supg 1 -supg -i_supgpre 3 -no-fd -no-local


7/20
good results:
srun -N 32 -n 512  --ntasks-per-node=16 ../imMHDp -m ../../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 8. -dt .05 -usepetsc --petscopts ../rc_debug4 -s 3 -shell -resi 1e-5 -visc 1e-5 -im_supg 1 -supg -i_supgpre 3 -no-fd -local -lr 5 -visit

2/2^11

7/17
after fix, it can run to t=6.7 and I think 6.8 is the first peak for 1e-5, it is challenging

memory leakage:

try this:
srun -n 36 ./imMHDp -m ./Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf .4 -dt .1 -usepetsc --petscopts ./petscrc/rc_debu -s 3 -shell -resi 1e-5 -visc 1e-5 -im_supg 1 -supg -i_supgpre 3 -no-fd -tchange .2

srun -n 36 valgrind --leak-check=yes --track-origins=yes --log-file="log-valgrind1.out" ./imMHDp -m ./Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf .4 -dt .1 -usepetsc --petscopts ./petscrc/rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 -im_supg 1 -supg -i_supgpre 3 -no-fd -tchange .2

7/16
conclusions:
0. tau is okay

1. supg is doing the right thing (the system is nicer, nonlinear residual decays monotone, it does not need many nonlinear iteration to converge, but the physics-based preconditioner does not like that, typically 300-400 linear iterations for 5-6 Newton iterations); so we need to improve the preconditioner for hard problems
>>> compare with two other implementations of supg, I think my implementation is fine
>>> with supg, the Schur complement and ARe works well for res=1e-5
>>> however we may need to improve our preconditioner w.r.t. supg, this may not be easy; In supg/pspg Navier-Stokes case, they keep almost every terms from supg

2. if the mesh is packed correctly, the simulation can run through the simulation with 1e-5

3. I will try 1e-6

4. there is a chance we could use pAIR if needed 


at least r4rl4 can run to t=8 with supg, so there is hope

todo:
check supg if adding terms into it
push resolution and resisitivity
add more terms for supg



7/15
explicit scheme
srun -n 72 ./imMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 100 -tf .1 -dt .001  -resi 1e-5 -visc 1e-5 -ex_supg 1 -stab -s 2 -itau 3 -no-fd -local -lr 5

srun -n 6 ./imMHDp -m ./Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 100 -tf .1 -dt .001  -resi 1e-5 -visc 1e-5 -ex_supg 1 -stab -s 2 -itau 3 

7/14
questions: 
1. does 1e-4 works on finer mesh?
todo
2. is there a mesh 1e-5 working?
yes, but still underresolved
3. small time step for implicit scheme (not good, this is strange)

7/12
srun -n 72  ../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 1. -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 -supg -im_supg 1 -no-fd -local -lr 2

tau=0.0146444 Unorm=7.35696e-07  h=0.03125??

try the same resolution on both res? (fine and coarse)

coarse case:
srun -n 72  ../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 8. -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 -supg -im_supg 1 -no-fd -local -lr 2

The preconditioner struggles after t=3.3, it is not clear if it is underresolved or just the preconditioner becomes bad

when preconditioner becomes inefficient, v is still small, less than 0.1?

this becomes significantly worse than 1e-4!

this was the starting point:
https://github.com/mfem/mfem/blob/pumi-NS/fem/NSnonlininteg.cpp

how about only add the diffusion term? I have a smaller dt??
srun -n 72  ../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 8. -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 -supg -im_supg 1 -no-fd -local -lr 2

the trend: the preconditioner struggles if supg diffusion is larger!

7/9
Things to try:
1. small time step to run thourgh (with the dt term ignored)
??

2. run the same simulation with the same mesh for 1e-5
it works for coarser meshes but finer meshes struggles? why? maybe if we can fix the issue w.r.t. stabilization terms, it would be better

3. change ||v|| to local instead of max
this was the case

7/6
supg optimization
mpirun -n 48  ../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 1. -dt .1 -usepetsc --petscopts ../petscrc/rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4 -no-supg -no-fd -local -lr 2
vs
mpirun -n 48  ../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 1. -dt .1 -usepetsc --petscopts ../petscrc/rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4 -supg -no-fd -local -lr 2
vs
mpirun -n 48  ../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 1. -dt .1 -usepetsc --petscopts ../petscrc/rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4 -supg -im_supg 1 -no-fd -local -lr 2

first try supg on y2 only; preconditioner good earlier time and it can run through
then try add supg on both terms but no Nb term; precondition works slightly worse (ok up to t=8)
fails t=2.5 if Nb term turned on 

6/23
compare later time with other terms in the diagonal 

6/22
optimize this for supg preconditioner:
mpirun -n 12 --oversubscribe ../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 1. -dt .1 -usepetsc --petscopts ../petscrc/rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4 -no-supg -no-fd -local -lr 2
vs
mpirun -n 12 --oversubscribe ../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 1. -dt .1 -usepetsc --petscopts ../petscrc/rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4 -supg -no-fd -local -lr 2

this was not good with supg alone (after two time steps):
srun -n 72 ../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 1. -dt .1 -usepetsc --petscopts ../petscrc/rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 -no-supg -no-fd -local -lr 2

mpirun -n 48 ./imMHDp -m ./Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 1. -dt .1 -usepetsc --petscopts ../petscrc/rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 -supg -no-fd -local -lr 2

preconditioner ok with only (v.grad)^2 term
adding extra mass terms into supg is not good?

what about supg on omega? any preconditioner on that would help?? a simple addition to ARe not very good, but is it better than doing nothing?? very little

adding supg only to ARe? this is close to doing nothing?

6/14
question to investigate:
increase rtol will be acceptable? maybe; it looks okay
increase grid resolution at the center? 
reduce time step defintely help but it still get stuck around t=4.8? what does solution look like? too large speed?

r5lr4 always got stuck around t=4.5 (with dt=0.1)
the cause seems the preconditioner becomes inefficient (how could I improve that?)

maybe because when v is getting large ARe/Sc is not inverted very efficiently??
ksp iterations increase to achieve the same order of rtol
play with parameters in hypre to see!

r5lr5 will push time to later??


increase mffd_err x
decrease mffd_err x

increase ijacobi x
decrease ijacobi x

increase rtol0 helps r5lr4

change bt to l2 as line search ok

it is possible for rl5 I need to increase rtol0

this hypre flag may help?
-s2_pc_hypre_boomeramg_interp_type FF1 

ARe and Sc is the issue!!
Q: a better way to invert that matrix: AIR?
Q: supg will save me?

see logr5-lr4-9no-stab.out


6/10
l2 linesearch is better if problems are getting harder (or more cores are called)

6/8
1. it is getting better if we improved stiffness matrix inversion

without stabilization (not necessary for res=1e-4, even 1e-5; and if turned on, the preconditioner may struggle)
2. o=3 and res=1e-4 (local refine levels=2)
   ***  r4rl2o3 works (dx_min=.0078; see test-res1e-4/r4lr2o3)
        pics: one grids points in the layers
   ***  r5rl2o3 not working, why? it got stuck with rtol=1e-3 or so (dx_min=.0039)
                increase mffd? 
                decrease small matrix rtol? 
                decrease rtol_0 in inexact-Newton (It works. run to the end; see test-res1e-5/r5rl2o3)
        pics: two grids points in the layers
        Question: what's a better rtol0? Luis used rtol_0=0.8?? For this case, 0.1 is better than 0.3

   ***  o=3 works for a stretched mesh: minimal grid spacing: 0.25/2^5=.0078; it runs really well

3. o=4 seems working for res=1e-4 
   o=4 with stretched grids okay (after changing mffd to 5e-2)

4. stabilization term seem could be another issue

5. o=3 and res=1e-5 without stabilization
    *** r5rl2o3 (with stabilization terms is bad: adjust rc_debug?)
    *** r5rl2o3 (without stabilization; it runs to t=4.2)
    *** r5rl3o3 (without stabilization; it runs to t=4.5)
    *** r5rl4o3 (without stabilization; it runs to t=4.5)
        rtol=1e-3 is better?


6. next: try to do local refine to get 1e-5 

7. supg preconditioner? 2016 paper; 2003 JCP
supg may be better than hyperdiffusion

8. next: amr (probably better; 1e-4 was okay)

this was working (same dx_min):
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 10


6/4
* BDF may be possible
* res 1e-4 okay
* res 1e-5 has some trouble to run to the end but it may be fixable (sensitive to all the parameters we have in the system--paramters in the preconditioner; mesh; mffd; order; hyperdiffusion)
* will do amr later

>>> possible comparison: 1e-3 adaptive vs uniform

only supg is not good (probably need to add it into the preconditioner)
supg+fd is okay
no-supg and no fd fails later at t=5.* for 1e-5

try increase the stiffness tightness case3adaptiveR5lr1Res1e-5-no-stab/log-tf-stiff.out

maybe I need to add supg into the preconditioner?

6/2
*BDF possibility?
bdf is available through petsc integrator but it means I need to add one more level of TS solve, which is probably the easiest though
bdf is also available through sundial but it may not be a good idea to have a mixed petsc and sundial interface. So we need to use kinsol 
native implementation of bdf may not be easy

res 1e-4
this seems okay:
srun -n 72 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -no-vis -vs 1 -tf 5. -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4 -no-supg -no-fd


srun -n 72 ../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 1. -dt .1 -usepetsc --petscopts ../petscrc/rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4 -no-supg -no-fd -local -lr 1

o3 is okay after adjust the rc_debug

this fails:
srun -n 72 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -no-vis -vs 1 -tf 5. -dt .1 -usepetsc --petscopts ../petscrc/rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4 -no-supg -no-fd > log-no-stab.out


res 1e-5
case3StretchedR5o4res1e-5 stuck at t=4.6 (probably underresolved)

this is okay (40 linear solve)
srun -n 72 ../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 1. -dt .1 -usepetsc --petscopts ../petscrc/rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 -supg -fd -local -lr 1

as a comparison (a nonconforming grid is better than stretched grid; so amr may be okay):
srun -n 72 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -vs 1 -tf 5. -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 -supg -fd

the best it runs to t=4 (it was much longer). too many parameters to adjust...

see case3adaptiveR5lr1Res1e-5-new for instance


a bug in the current? fixed


5/28
run more tests

this is working (tfinal=4 with dt=.1):

(it is better than one level of meshes without a coarse mesh layer)

3/22
Try supg on res=1e-3 to see if it is working?? Maybe supg preconditioner is needed there!!

3/17
without supg and fd, it works with
srun -n 72 ./imMHDp -m ./Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 1 -tf .2 -dt .1 -usepetsc --petscopts ./petscrc/rc_full -s 3 -shell -resi 1e-6 -visc 1e-6 -no-supg -no-fd -local -lr 2

but this was not working:
srun -n 128 ./imMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 2 -tf .2 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6 -supg -fd -local -yrefine 0.2 -part_method 1 

why??
two possibilities: rc_full or supg or fd, maybe because of J boundary??
It looks like it is an issue with fd!! Maybe reduce alpha??

choose alpha=0.0001 still not working, maybe modify the preconditioner??

ok, the boundary issue is clearly the killer!

this gives me crappy omege (not other solutions though):
mpirun -n 72 ../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 6 -no-vis -tf 1 -dt .1 -usepetsc --petscopts rc_full -s 3 -shell -resi 1e-6 -visc 1e-6 -supg -fd -local -lr 2 -alpha 0.0001

Maybe fd does not like jumps in diffusion?

3/16
mpirun -n 12 ./imMHDp -m ./Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -tf .2 -dt .1 -usepetsc --petscopts petscrc/rc_full -s 3 -shell -resi 1e-4 -visc 1e-4 -no-supg -no-fd -updatej 0

for icase=5 supg and fd need some work, but it does not work very well
Also tried Neumann boundary condition for omega, it does not gives better solutions.

3/12
This produces the same result with the smoother perturbation (icase=4):
srun -n 72 ./imMHDp -m ./Meshes/xperiodic-new.mesh -rs 6 -rp 0 -o 3 -i 4 -no-vis -tf 4 -dt .1 -usepetsc --petscopts petscrc/rc_full -s 3 -shell -resi 1e-4 -visc 1e-4 -no-supg -no-fd -updatej 0

srun -n 72 ../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 8 -rp 0 -o 3 -i 4 -no-vis -tf .2 -dt .1 -usepetsc --petscopts rc_full -s 3 -shell -resi 1e-4 -visc 1e-4 -no-supg -no-fd -updatej 0


3/11
Compare two solvers (coarse mesh they are comparable):
srun -n 36 imMHDp-old -m ./Meshes/xperiodic-new.mesh -rs 7 -rp 0 -o 2 -i 3 -vs 2 -tf .2 -dt .1 -usepetsc --petscopts ./petscrc/rc_full -s 3 -shell -resi 1e-4 -visc 1e-4 -no-supg -no-fd

After the boundary is fixed (Dirichlet), rs=9 o=2 works fine for res=1e-4 and visc=1e-4!!

rs=9 o=3 struggles, maybe reduce the time step, or change mffd?
What if I reduce rs??

>>>res=1e-4
rs=7 o=3 mffd=5e-2 works
rs=8 o=3 mffd=? I tried 1e-1 to 1e-3, not working so far... dt=0.01 working but that is terrible!! What if I change the problem to remove the "boundary layer"? Yes, it passed the first dt=0.2

rs=9 o=3 mffd=?


3/9

0. Implemented supg, it gets better but it could not passed the first peak for res=1e-6. See (why under-resolved?)
/net/scratch3/qtang/mfem.p/runs/case3r5o3-HolderRes1e-6/rs4t5
I refined, but the solver fails at an even earlier time. Need preconditioner on supg?

Supg and FD do not like stretched grid, see 
/net/scratch3/qtang/mfem.p/runs/case3r5o3res1e-6-stab-w-t6

1. I found the preconditioner does not like the "auxiliary" formulation of J, because they are inconsistent with how J is kupdated. can we use Dirichlet on J on the boundary? 

2. The boundary condition still has the question mark. I think that may affect preconditioner when res is small?? 

(Maybe fixed after fixing the boundary) 3. The preconditioner is sensitive to the choice of all the parameters


Two more possible fixes:
1. Add preconditioner for supg
2. Add the more regularity into the solution (diffusion for omega)

3/8
When o=2 and resi=1e-4, the solution can be computed without supg or fd to t=8
* Note when o=2, hyper diffusion becomes very bad!

In general, when it fails, I am not sure if it is because I pick the wrong preconditioner parameters or the solver actually fails...

For instance, for a resolution of 512*512 (under-resolved!!) and o=2,
rtol0=0.3
mffd=1e-2
Not converged at t=2...

rtol0=0.1
mffd=1e-3
Converged!!

That could explain why my solver fails to converge!!

When R=9, now it is not converged for res=1e-4?? Figure that out!!

Is the matrix symmetric?? Yes

3/5
Q1:
* I think the fd may be the killer for the solver; let's turn that off and see what happen (case3r5o3res1e-4visc1e-6) 
  Yes, without fd, it actually works well and run up to t=4.4
  With fd; it does not work with this viscosity, why? Maybe reduce alpha?

Q2:
  It also does not like the double refined mesh? Why? Even 1e-4 1e-4 without any stabilization terms! It appears to indicate I need to reduce my time step!!!
  The equivalent mesh is 512*512. The adaptive mesh is not good...

  Let's change rc_debug to see. Maybe change refined region later?

  I start to redo the uniform mesh with supg and fd turned off. It appears not convergent if I pick a wrong rc_debug
  So it seems rc_debug can be sensitive! (If I mess up rc_debug in o=2, it also fails)

Q3:
* try supg with full stabilized terms on both omega and psi; is it better for res=1e-5 visc=1e-5?

3/3
Use R1 base mesh:
r6+local1: it stops after t=3.9
r5+local1: it fails after t=5.1
But why? It is not due to supg since v is small, maybe we should reduce time step, or maybe we should reduce eps in jfnk??
r=6+local=1 means resolution of 2^10

So let us add one more level (it fails with R0+r5+local2? Ugh..)

When the mesh is finer, reduce preconditioner tol seems help?! Maybe I should play with rc_debug than doing more levels of refinement?

OK, things become so strange. Let's move back to 1e-5. 
It got stuck again at an earlier time? 

Test this:
../imMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 2 -tf 1 -dt .1 -usepetsc --petscopts ../petscrc/rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 -supg -fd -local -lr 1

***Should we turn off supg on psi?? ***

This runs got stuck at t=4.*
If no supg, it got stuck earlier t=3.8!

For res=1e-5+vis=1e-5:
with supg, it got stuck at t=4.4

For res=1e-6+vis=1e-6
With supg, it got stuck at t=5.1

Two possibility: 
1. We need to provide a preconditioner for supg terms;
2. We need more regularities in finite element

 

3/2
New mesh: stretched64x64.mesh the smallest cell size: 0.0112 largest is 0.1376

Failed to converge with field-line diffusion; it probably does not like the large ratio between two cells

Uniform: r=6 is comparable to r4 with xperiodicR1 (maybe stick with this mesh for now)

Both have no problems to run up to t=4 with supg and fd (with stretched grid it is not good)

r=6 it got stuck after t=5.1

Replace it with an adaptive version of supg

r=6: without fd; it fails to converge!
r=6: with fd and supg: it runs to t=5.1 until it becomes underresolved


2/27
Testing supg without fd-fem how will it do on stretched grid??

It looks like the preconditioner does not like a stretched grid?

Let's focus on this run:
../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 2 -tf 5 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6

fd vs supg vs none

Isn't it strange if I have stretched grid to compute eleLength?? Maybe that is the reason? But it is only off by a factor of 2 (it explains why if I stretched aggressively, things are not working).

At t=4; use supg (only on omega) and fd on (psi) gives a good solution

-no-fd, the solutions with supg become very bad (omega). The original bad results are probably due to under resolved.

-fd -no-supg is bad 

So supg really helps!!

Here is what is found at t=4:
No-supg fd: unstable at t=3.5
supg fd: good (adding supg into w only gives a slightly better result)
no-supg no-fd: converges fastest but bad results
supg no-fd: also bad results; supg is not strong enough (struggles to converge at a later time)

With-supg and with-fd: it gets stuck at t=4.6 (probably because it is under resolved)



2/26
OMG, I truned off the stabilization term when testing stretched grid!!

So the stretched grid works well for 1e-4 without stabilization term (in other words we could capture it through amr)!
the preconditioner with the stabilization term does not like the stretched grid!! Maybe adjust the fd term in jfnk?

res=1e-4 preconditioner does not like stretched grid; 
res=1e-6 it is better

uniform r5o3 stuck at t=5.6 (add more supg!)

xperiodic-stretched.mesh r5o3


2/21
two possibilities where the preconditioner fails:
Sc  (too oscillatory for omega, or underresolved? see case3StretchedR5o3res1e-5/log_study)
ARe (also too oscillatory omega or maybe underresolved??) see case3StretchedR5o5res1e-6 (no plot) or case3R1StretchedR5o3res1e-5 (no plot yet)

case3StretchedR5o3res1e-5
vs
case3R1StretchedR5o3res1e-5

Good results (1e-4):
case3stretchedR5o4

1e-6
xperiodic-newR1-stretched.mesh -rs 5 -rp 0 -o 3 with dt=0.1:
time=1924.11

what to do?
1. do more refinement study
2. do more tests with larger nu

2/20
mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -no-vis -vs 1 -tf 4.9 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6 > log3.out

1e-6
r5o4 stuck at t=5.0 (probably under-resolved)
r5o5 ??

1e-6 (more stretched)
R1r5o3 failed to converged!!

newR1r5o3 also fails to converge

1e-5

what about resistive=1e-5 and viscosity=1e-4

1e-6
r6o4 runs fine to t=2


some ideas to try:
do more stretching (this appears to be a bad idea R1-stretched is bad idea, maybe made the stretching more smooth??)
try add the B diffusion on omega



2/7
mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 5 -i 3 -no-vis -vs 1 -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6 > log1.out &

mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 1 -o 3 -i 3 -no-vis -vs 1 -tf 1 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 > log1.out &

mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 1 -o 4 -i 3 -no-vis -vs 1 -tf 1 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 > logR6o4.out

mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -no-vis -vs 1 -tf 4.9 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6 >log2.out

2/6
testing amr criterion:
(stuck at t=6.8)
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-8 -derefine -resi 1e-4 -visc 1e-4 -refs 4

mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-8 -derefine -resi 1e-4 -visc 1e-4 -refs 8

mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 1 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-8 -derefine -resi 1e-6 -visc 1e-6 -refs 8

mpirun -n 42 imMHDp -m Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -vs 2 -tf 2 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4 

mpirun -n 42 imMHDp -m Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -vs 2 -tf 2 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6


mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -vs 2 -tf 2 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4

mpirun -n 42 imMHDp -m Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -vs 2 -tf 2 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6

mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -no-vis -vs 1 -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4

mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -no-vis -vs 1 -tf 2 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5

this looks good at t=2
case3StretchedR5o4res1e-5

issue at t=5.4!! (this is underresolved)
case3StretchedR5o3res1e-5

ok:
case3StretchedR5o3
case3stretchedR5o4

2/5
mpirun -n 48 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 1 -o 3 -i 3 -no-vis -tf 5 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 1 -ltol 1e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

oscillations here??
mpirun -n 12 exMHDp -m Meshes/xperiodic-new.mesh -rs 5 -o 3 -tf 1 -i 3 -vs 100 -dt .001 -resi 1e-6 -visc 1e-6

2/2
ideas to try: 
1. no supg, resi=1e-4? t=8? but with amr
this can run but supg definitely helps a lot!

2. turn of supg after a while?
3. reduce supg for resi=1e-4

4. try to check if reduce nonconforming level would help??
set nc_limit=1 seems help a lot!!
resi=1-4 it can run up to t=6

mpirun -n 32 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 10

np=32 got stuck at t=7.2 (probably the second peak time!)

it reaches the peack around t=6.7-6.8

this works well:
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 10

this got stuck at t=4.8, wth? (something happens at this time??)
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-5 -visc 1e-5 -refs 10

np=32 it got stuck at t=4.0
np=42 r=5, it got stuck at t=2.5, maybe increase wp to 5e-3
np=42,r=5, after modification, it got stuck at 4.4

try this (stuck at t=3):
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 2 -tf 4 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 5e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 4 -yrange

mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 2 -tf 4 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 5e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

(this got stuck at t=4 after mesh is refined; the adding a fine mesh is a bad idea; can we do some prepacking??)
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 5e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

try (got stuck again at t=4; can we do prepacking??):
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

this seems got stuck at t=3.2
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 6 -rp 0 -o 3 -i 3 -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 5e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

try this:
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 6 -rp 0 -o 3 -i 3 -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 2 -ltol 1e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

try this:
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 7 -rp 0 -o 3 -i 3 -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 1 -ltol 1e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

try this (this is fine but very under-resolved):
mpirun -n 42 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 2 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4 

1/29
mpirun -n 46 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 5 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 4
this got stuck at t=2.0?? with rc_debug

mpirun -n 46 imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 5 -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 4

preconditioner becomes worse after derefine or mesh changed in general??

okay (the plotting routine will affect AMR and may affect convergence, a bug???):
mpirun -n 44 ../imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 1 -tf 4 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 10

mpirun -n 48 ../imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 10 > log.out

1/22
try to investigate why the hyprediffusion is so stiff in an explicit scheme (1e-6 for resistivity)
mpirun -n 12 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 100 -tf 4 -dt .0005 -resi 1e-6 -visc 1e-6


explicit scheme without FD is stable for dt=.008 with rs=3
explicit scheme without FD is stable for dt=.004 with rs=4
explicit scheme without FD is stable for dt=.002 with rs=5
explicit scheme without FD is stable for dt=.001 with rs=6


with the diffusion term, it is not working with dt=.001 with rs=4 and alpha=.2

it becomes unstable somewhere between t = .72 and .73
srun -n 36 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -tf .75 -dt .0005 -resi 1e-6 -visc 1e-6

dt=.0002 seems okay
dt=.0004 seems okay (10 times smaller)


1/10
study FDFEM

tau = h^2/invtau;
this needs a much smaller time step for explicit scheme
dt=1e-6

use tau = .0001/invtau and a smaller time step for the hyprediffusion term (works much better):
mpirun -n 12 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 100 -tf 4 -dt .0005 -resi 1e-6 -visc 1e-6

can it run longer? Yes for the explicit scheme
will other terms matter? No.

Now consider the implicit case:
mpirun -n 12 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell

if ALPHA=1e-2; the preconditioner not good for resi=1e-6
if ALPHA=1e-3; the preconditioner seems ok

ALPHA=1e-3 corresponding to hyprediffusion parameter of 1.5e-8
In explicit schemes, diffusion paramter was 2.5e-8

ALPHA=5e-3 with dt=.05; preconditioner seems work
ALPHA=1e-2 with dt=.05; preconditioner seems work hyperdiffusion of 1.2e-7
ALPHA=5e-2 with dt=.05; preconditioner seems work hyperdiffusion of 6e-7 (80-100 iterations)

with o=2 things become strange with the stabilzed term turned on
mpirun -n 12 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 2 -i 3 -vs 2 -tf 1 -dt .05 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6

it is probably a bad idea to use hyperdiffusion p2


it is okay to use resi=1e-3 (30-40 iterations in total), but not resi=1e-6




12/23
this is not quite good even with supg (both element max or a local value)
imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 100 -tf 1 -dt .002 -resi 1e-6 -visc 1e-6

12/22
note that different resi gives different E0rhs!
at least stabilization terms has impact now but the stabilization parameter is funny

12/12
does the explicit supg make sense? I am not sure now...
maybe remove all other operators and only use (V.grad u, V.grad h) to make sure diffusion is doing something
dt/2(V.grad u, V.grad h) should be a reasonable diffusion term

12/10
testing:
mpirun -n 4 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 100 -tf .002 -dt .002 -resi 1e-6 -visc 1e-6

12/6
some notes for the operators:
grad(u)=J^{-T} hat_grad(hat_u)
J^{-1}=(1/det(J)) adj(J)

grad(u)*v*Weight()
but Weight()=det(J) and therefore only adjJ is needed

12/1
MatZeroRowsColumnsIS is eliminating zeros in my jacobi? I assume I could skip it if I use EliminateEssentialBC through mfem?
Should not I used a derived class from PetscBCHandler? So all those values should be protected for easier access?

11/20
there is a bug in the boundary condition in the implicit case (I probaby need to use bchandler).
[I 0] |x| = |a| 
[D A] |y|   |b|
is not necessary equivalent to
[0 0] |x| = |0| 
[D A] |y|   |b|
by setting the initial guess being x=a

11/19
resi=1e-4 imAMR the first run stuck at t=6.8 (the mesh looks good; but it probably needs slightly more damping)

mpirun -n 32 ../imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 5 -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 10

resi=1e-5 very oscillatory; the solve got stuck at 4.2
mpirun -n 32 ../imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 5 -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-5 -visc 1e-5 -refs 10

mpirun -n 32 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 5 -tf 1 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5

low-order
mpirun -n 42 ../imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 2 -i 3 -vs 5 -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-4 -derefine -resi 1e-6 -visc 1e-3 -refs 10

11/18
debug:
-m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -tf .5 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3

a bug fixed in ParBilinearForm define

this is ok (using refine step=4)
mpirun -n 16 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 1 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3

this is ok (I may try a fine mesh to start with instead; here the derefine is aggressive):
mpirun -n 16 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 1 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine


11/16
avoid SetFromTrueVector() when mesh is changed; this gives the wrong answer!


11/15
testing:
mpirun -n 4 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 1 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3


11/13
add a fixed adaptive mesh to imAMRMHDp;

this is okay
mpirun -n 4 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 2 -i 3 -vs 1 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell

this is okay for a fixed adaptive mesh (no significant impact)
mpirun -n 4 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 1 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3


11/4
move back to imAMRMHDp

things need to be changed: 
*1. j0 is owned in main (so that j0 can be updated in AMRUpdate)
*2. add j in main for amr estimator
3. initialize reducedSystemOperator after mesh is changed


9/16
for sl=10^3 and o=3, I cannot solve preconditioner too tight:
    for r=6, 7, 8
    mffd 1e-3
    rtol=1e-3 in small matrices is better than 1e-5 
    mass matrix should always be 1e-6

    for r=7, resi=1e-4
    it is more important to use 1e-2 in mffd
    for r=8, resi=1e-4
    I cannot find a working parameter set so far
    what if I recuded mffd to 5e-2

I am wondering if it could help o=2 s=10 case
1e-3 is okay for r=7, r=8, not okay for r=9

8/27
revisit exAMR

this seems working
exAMRMHD -m Meshes/xperiodicR3.mesh -r 0 -o 3 -tf 8 -vs 10 -dt .0002 -i 3 -amrl 3 -ltol 1e-3

o=3 derefine seems have very small impact;
exAMRMHD -m Meshes/xperiodicR3.mesh -r 0 -o 3 -tf 8 -vs 10 -dt .0002 -i 3 -amrl 3 -ltol 1e-3 -derefine

this is okay np=4,8 (it is possible because the previous AMR setup has some issue; it could be derefine was too aggressive):
mpirun -np 4 exAMRMHDp -m Meshes/xperiodicR3.mesh -rs 0 -o 3 -tf 1 -vs 10 -dt .0002 -i 3 -amrl 3 -ltol 1e-3 -derefine

this becomes strange (this is not the same because derefine is different!!)
mpirun -np 4 exAMRMHDp -m Meshes/xperiodic.mesh -rs 3 -o 3 -tf 1 -vs 10 -dt .0002 -i 3 -amrl 3 -ltol 1e-3




o2 is not working

8/26
exAMRMHD fails for o=3? Diverged pcg? time steps are probably too large

8/24
test rs=9: will reduce stiffness rtol=1e-5 be better?

8/23
resi=1e-3 is okay for rs 7, 8 not 9
resi=1e-4 is okay for rs 7, 8, 9

for resi=1e-4
for rs<=7 rtol=1e-5 is good
for rs>7 rtol for Schur needs to be 1e-6

rs=9
change stiffness to 1e-5 ok
mass matrix has to be inverted accurately

iter=2,3 not good


8/20
mpirun -n 16 imMHDp -m Meshes/xperiodic-new.mesh -rs 6 -rp 0 -o 2 -i 3 -vs 1 -tf 1 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell

potential improvements (minor):
1. do schur complement inside petsc
2. not assemble matrix when impose boundary condition

8/19
this seems work:
srun -n 16 imMHDp -m Meshes/xperiodic-square-new.mesh -rs 6 -rp 0 -o 2 -i 2 -vs 1 -tf 5 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell
srun -n 16 imMHDp -m Meshes/xperiodic-square-new.mesh -rs 7 -rp 0 -o 2 -i 2 -vs 1 -tf 5 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell
srun -n 32 imMHDp -m Meshes/xperiodic-square-new.mesh -rs 8 -rp 0 -o 2 -i 2 -vs 1 -tf 5 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell


8/17
new tests:
mpirun -n 4 exMHDp -m Meshes/xperiodic-new.mesh -rs 4 -o 2 -tf 1 -vs 100 -dt .001 -i 3
mpirun -n 4 exMHDp -m Meshes/xperiodic-square-new.mesh -rs 2 -o 2 -tf 3 -vs 100 -dt .001

redo:
64*64
mpirun -n 4 imMHDp  -m Meshes/xperiodic-square-new.mesh -rs 4 -o 2 -i 2 -tf 10 -dt 5 -s 3 -usepetsc --petscopts petscrc/rc_full -shell

test:
srun -n 16 imMHDp -m Meshes/xperiodic-square-new.mesh -rs 6 -rp 0 -o 2 -i 2 -vs 1 -tf 10 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell
mffd_err=1e-3
r=6 (256x256): iter=60 to 80
mffd_err=1e-3
r=7 (512x512): iter=134, 113, 105, 143; some norms are off
mffd_err=1e-2
r=7 (512x512): iter=80, 103, 107, 143; norms are close
r=8 : iter=187, , , ; norms are close
mffd_err=1e-1
r=8 : iter=130, 155, 146, ; norms are close

-mat_mffd_err 1e-1 -mat_mffd_type ds -mat_mffd_umin 1e-5 
r=7 (512x512): iter=112, not working...

from info?? 
[0] MatMult_MFFD(): Current differencing parameter: 6.962523027760e-09
[0] MatMult_MFFD(): Current differencing parameter: 2.757792494633e+00

will o3 scale better?



8/13
srun -n 16 imMHDp -rs 6 -rp 1 -o 2 -i 2 -vs 1 -tf 5 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell
key parameters:
-snes_linesearch_type bt
-mat_mffd_type wp
-mat_mffd_err 1e-3
-snes_ksp_ew_rtol0 0.3
-snes_ksp_ew_rtolmax 0.9

r=8 can be solved by:
-mat_mffd_err 1e-2

8/11
try this:
mpirun -n 4 imMHDp -rs 5 -o 3 -i 2 -tf 10 -dt 2 -vs 1 -no-vis -usepetsc --petscopts petscrc/rc_debug -s 1 -shell
snes_rtol=1e-4 amg tol=1e-5
time 2281

add SDIRK and mid-point stepping

this is okay (change mesh to a single mesh)

ok
mpirun -n 6 imMHDp -m meshes/xperiodic1.mesh -rs 4 -rp 2 -o 3 -i 3 -vs 1 -tf .1 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell
  total number of linear solver iterations=15

not ok
mpirun -n 6 imMHDp -m meshes/xperiodic1.mesh -rs 5 -rp 2 -o 3 -i 3 -vs 1 -tf .1 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell

ok
mpirun -n 4 imMHDp -rs 4 -rp 2 -o 2 -i 2 -vs 1 -tf 5 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell

ok
mpirun -n 6 imMHDp -rs 5 -rp 2 -o 2 -i 2 -vs 1 -tf 1 -dt 1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell
let mffd be 1e-4 becomes much better?!

check the allocation part, is it slow??
mpirun -n 6 imMHDp -rs 5 -rp 3 -o 2 -i 2 -vs 1 -tf 1 -dt 1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell

mat_mffd_err 1e-4 works fine for r=7
mat_mffd_err 1e-3 works fine for r=8! it works for dt=5 of r=7!


8/8
probably need to change Sc to nontranspose version? No
Changing mat_mffd_err 1e-5 helps significantly!

this works:
 mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -vs 1 -tf 250 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 1 -shell
 mpirun -n 4 imMHDp -m xperiodic.mesh -rs 4 -o 3 -i 3 -vs 1 -tf 8 -dt .2 -usepetsc --petscopts petscrc/rc_debug -s 1 -shell

use richardson as ksp if one wants to use hypre as the solver in petsc

8/7
inexac Newton?

preconditioner needs improvements in solving omega (increase regularity)

this does not perform well
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -vs 1 -tf 5 -dt 5 -usepetsc --petscopts rc_debug -s 1 -shell

inexact newton is ok for case 1
mpirun -n 4 imMHDp -rs 4 -o 2 -vs 1 -tf 3 -dt .1 -usepetsc --petscopts rc_debug -s 1 -shell
ok for case 3
mpirun -n 4 imMHDp -m xperiodic.mesh -rs 4 -rp 0 -o 3 -tf 1 -vs 1 -dt .1 -i 3 --usepetsc --petscopts rc_debug -s 1 -shell

icase 2:
resitivity=10^4; dt = 2 ok
resitivity=10^3; dt = 1 ok
resitivity=10^3; dt = 2 (fgmres is slightly better; will take more ksp solves)

maybe I should use MatCreateSchurComplement?

8/6
compare
run -n 4 imMHDp -rs 2 -o 2 -tf 3 -vs 100 -dt .002 -usepetsc --petscopts rc_jfnk -s 1 -shell
vs
run -n 4 imMHDp -rs 2 -o 2 -tf 3 -vs 100 -dt .002 -usepetsc --petscopts rc_full -s 1 -shell

this seems working:
srun -n 4 imMHDp -rs 4 -o 2 -tf 3 -vs 100 -dt .1 -usepetsc --petscopts rc_full -s 1 -shell

when iter=10, whis works
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -tf 1 -dt .02 -usepetsc --petscopts rc_full -s 1 -shell

when iter=5, this is ok
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -tf 1 -dt .01 -usepetsc --petscopts rc_full -s 1 -shell

when iter=8, this is ok
mpirun -n 4 imMHDp -m xperiodic.mesh -rs 4 -rp 0 -o 3 -tf 1 -vs 50 -dt .01 -i 3 --usepetsc --petscopts rc_full -s 1 -shell

My time step is smaller than pixie2d??

Possible fix:
1 use fieldsplit
2 use inexact Newton

I am doing GMRES for nested mat or jfnk?? jfnk

iter=5 is ok if GMRES is turned on but I am not sure I am doing the correct GMRES

this works (with iter=5):
-snes_monitor
-snes_mf_operator
-snes_max_it 20
-snes_rtol 5e-6
-snes_converged_reason 
-snes_linesearch_type l2
#
# use inexact newton based on pixie2d:  
# ierr = SNESKSPSetParametersEW(snes,3,tolgm,0.9,0.9,1.5,1.5,0.1)
-snes_ksp_ew
-snes_ksp_ew_version 3
-snes_ksp_ew_rtol0 5e-6
-snes_ksp_ew_rtolmax 0.9
-snes_ksp_ew_gamma 0.9
-snes_ksp_ew_alpha2 1.5
-snes_ksp_ew_alpha2 1.5
-snes_ksp_ew_threshold 0.1
#
-ksp_type gmres
-ksp_converged_reason
-ksp_rtol 5e-6
-ksp_monitor_true_residual
#======Stiffness matrix======
-s1_ksp_rtol 1e-6
-s1_ksp_type cg
-s1_pc_type hypre
#======Schur matrix======
-s2_ksp_rtol 1e-6
-s2_pc_type hypre
#======Mass matrix======
-s3_ksp_rtol 1e-6
-s3_ksp_type cg

ok:
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -vs 1 -tf 20 -dt 1 -usepetsc --petscopts rc_full -s 1 -shell
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -vs 1 -tf 20 -dt 5 -usepetsc --petscopts rc_full -s 1 -shell

what kind of newton solver used in pixie2d?


8/5
The current way of allocating hypre matrices may be slow in assembly

Alternatively, I could try:
VecReciprocal(diag);
MatDiagonalSet
MatPtAP

Try use Add to define some matrices (in SetParameters)

7/29
fix useFull=false (I need to use ParAdd and I cannot delete BilinearForm)
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -tf 1 -dt .001 -usepetsc --petscopts rc_jfnk -s 1 -shell

ok:
srun -n 4 imMHDp -rs 4 -o 2 -i 2 -tf 1 -dt .001 -usepetsc --petscopts rc_jfnk -s 1 -shell
srun -n 4 imMHDp -rs 2 -o 2 -tf 3 -vs 100 -dt .002 -usepetsc --petscopts rc_jfnk -s 1 -shell

7/13
in GetGradient:
assemble Nv and Nb (let diag = 0)
construct Sc = ASl + Nb^T D^-1 Nb: follow ex5p
   M->GetDiag(*Md);
   MinvBt->InvScaleRows(*Md); 
and pass
ARe Nb 0
0   Sc 0
K   0  M

there is something wrong in the Jacobi iteration:
missed one right hand side!
missed the correction for the second equation in my Jacobi iteration!

Save Mdt+KSl and Mdt+KRe; adjust only when timestep is changed

working example:
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -tf 1 -dt .001 -usepetsc --petscopts rc_jfnk -s 1 -shell
vs
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -tf 1 -dt .001 -s 2 

6/24
working on full preconditioner: follow ex5p.cpp

option 1:
GetDiag for M/dt+SRe and GetGiag for Nv
option 2:
use MatGetDiagonal (eaiser)
DRe and DSl are assembled in parallel

Step 1: Try this instead (this will give me HypreParMat for diffusion operartor):
   S0->Assemble();
   S0->EliminateEssentialBC(ess_bdr);
   S0->Finalize();
   HypreParMatrix * matS0   = S0->ParallelAssemble();    delete S0;

Mult (HypreParVector &x, HypreParVector &y, double alpha=1.0, double beta=0.0)
   y = alpha * A * x + beta * y
So addMult should be
matS0.Mult( x, y, 1.0, 1.0)

then delete DSl DRe

tried but this is not going to work because ParallelAssemble removes all the terms w.r.t essential dofs. However, this should not affect the preconditioner

Step 2: double check notes; ok
Step 3: build two operators and add capibility of changing time step (variable time stepping is left for later)
Step 4: implement Jacobi iteration in pcshell


Nb->ParallelAssemble;
Md = new HypreParVector(MPI_COMM_WORLD, Nb->GetGlobalNumRows(),
                              Nb->GetRowStarts());
HypreParVector *Md = NULL;
M->GetDiag(*Md);
MinvBt->InvScaleRows(*Md);
S = ParMult(B, MinvBt);

the only question is Jacobi iteration. Maybe, I could rescale in pcshell
Yes, use VecPointwiseDivide to implemented Diag_Re

There are two ways to build Schur complement: in mfem or petsc. I will build it in mfem for now

How many Jacobi iterations? fixed of 2-4 (no need to set stoppage)
What would be a good stoppage critera for Jacobi iteration?

Be careful for the negative signs in the preconditioner!
Should I do something special for the boundary? 
=======
6/25
try to reproduce results:
srun -n 4 imMHDp -rs 4 -tf 1 -vs 200 -dt .001 -i 2 -no-vis

but 
EliminateEssentialBC(ess_bdr) will not generate the matrix I need

6/16
the explicit and implicit solvers are comparable with pcshell:
(this uses a very bad linear solver, so it converges very slow)

ok:
###
srun -n 1 ../imMHDp -rs 2 -o 2 -tf .05 -vs 100 -dt .001 -m ../xperiodic-square.mesh
###vs###
srun -n 1 imMHDp -rs 2 -o 2 -tf .05 -vs 40 -dt .00025 -usepetsc --petscopts rc_mfop -s 1 -no-vis
-snes_monitor
-snes_mf_operator
-snes_max_it 10
-snes_rtol 1e-5
-ksp_type minres
-ksp_rtol 1e-5
-ksp_atol 1e-20
-pc_type jacobi

ok:
###
srun -n 4 ../imMHDp -rs 4 -o 2 -tf .0001 -vs 100 -dt .0001 -m ../xperiodic-square.mesh
###vs###
srun -n 4 imMHDp -rs 4 -o 2 -tf .0001 -vs 40 -dt .00002 -usepetsc --petscopts rc_mfop -s 1 -no-vis


ok:
mpirun -n 4 ../imMHDp -rs 4 -tf .001 -vs 200 -dt .001 -i 2
srun -n 4 imMHDp -rs 4 -o 2 -tf .001 -dt .00002 -i 2 -usepetsc --petscopts rc_mfop -s 1 -no-vis
petsc flag:
-snes_monitor -snes_mf_operator
-snes_max_it 10
-snes_rtol 1e-5
-ksp_type minres
-ksp_rtol 1e-5
-ksp_atol 1e-20
-pc_type jacobi

not ok:
srun -n 1 imMHDp -rs 2 -o 2 -tf .05 -vs 40 -dt .00025 -usepetsc --petscopts rc_jfnk -s 1 -shell

sub is deleted in petsc!!

add a mini example:

this works fine:
srun -n 1 mini -rs 2 -tf .00004 -dt .00002 --petscopts rc_mfop
srun -n 4 mini -rs 4 -tf .00004 -dt .00002 --petscopts rc_mfop

this is not working:
srun -n 1 mini -rs 2 -tf .00004 -dt .00002 --petscopts rc_jfnk -shell
srun -n 4 mini -rs 4 -tf .00004 -dt .00002 --petscopts rc_jfnk -shell

counter part:
srun -n 1 imMHDp -rs 2 -o 2 -tf .00004 -dt .00002 -usepetsc --petscopts rc_jfnk -s 1 -shell
srun -n 4 imMHDp -rs 4 -o 2 -tf .00004 -dt .00002 -usepetsc --petscopts rc_jfnk -s 1 -shell


6/11
debug:
srun -n 1 imMHDp -rs 2 -o 2 -tf 3 -vs 100 -dt .002 -usepetsc --petscopts rc_jfnk -s 1 -no-vis




6/6
Is the iterative mode working for PetscNonlinearSolver??
I think so. However, preconditioner should not use iterative mode, since it solves for du!

6/5
PetscParMatrix *pA;
oh.Get(pA);
hypre_ParCSRMatrix *parcsr;
MatHYPREGetParCSR(*pA,&parcsr);

Could I just use MATAIJ in jacType? I do not see the point to use MATHYPRE
Do I construct a MyBlockSolver every time step or not? No

5/29
o ReducedSystemOperator 
o link it with petsc
o getgradient
o preconditionerfactory 
o add kspsolve into pcshell

class MyBlockSolver : public mfem::Solver
{
   MyBlockSolver(OperatorHandle oh) { //constructor
  PetscParMatrix *PP;
  // Get the PetscParMatrix out of oh. I showed you how to do it
  Mat P = *PP; // type cast to Petsc Mat
  MatNestGetSubMats(P,&N,&M,&sub)// sub is an N by M array of matrices
  // Create your own internal KSP objects to handle the subproblems
  // Create PetscParVectors as placeholders internal_x and internal_y
}
  Mult(const Vector & x, Vector &y)
  {
   internal_x.PlaceArray(x.GetData()); // no copy, only the data pointer is passed to PETSc
 
   internal_y.PlaceArray(y.GetData());


  MatNestGetIs(...) // get the index sets of the blocks
  for i = 1:3
    VecGetSubVector(internal_x,index_set[i],&blockx)
    VecGetSubVector(internal_y,index_set[i],&blocky)
   KSPSolve(kspblock[i],blockx,blocky)

    VecRestoreSubVector(internal_x,index_set[i],&blockx)
    VecRestoreSubVector(internal_y,index_set[i],&blocky)
  }


 
   internal_x.ResetArray();
 
   internal_y.ResetArray();



  }
}

Solver * PreconditionerFactory::NewPreconditioner(OperatorHandle oh)
{
  return new MyBlockSolver(oh);
}

Q:
Could I call mfem linear solver in reducedsystem::mult?
Is it better to pass pointers of HypreParMatrix to reducedsystem? I do not think reducedsystem owns those objects.
Similarly, I could pass M_solver into reducedsystem.

5/22
Try this (is that for parallel?)
   BlockOperator *darcyOp = new BlockOperator(block_trueOffsets);
   darcyOp->SetBlock(0,0, M);
   darcyOp->SetBlock(0,1, BT);
   darcyOp->SetBlock(1,0, B);

This is for serial:
   BlockMatrix A = new BlockMatrix( offsets );
   A->SetBlock(0,0, &A00);
   A->SetBlock(0,1,  &A01);
   A->SetBlock(1,0,  &A10);

In pcshell, I need to
    KSPSetOperators(ksp1,Mdt,Mdt);
    KSPSolve(ksp1, b2, x2)
    KSPSolve(ksp1, b3, x3)
    
    MatMult(Mdt,x3,bTmp);
    bTmp=bTmp*dt+b1;
    KSPSetOperators(ksp2,K,K);
    KSPSolve(ksp2, b2, x2)

See
file:///Users/qtang/software/petsc-3.10.2/src/ksp/ksp/examples/tutorials/ex9.c.html

Where should I initialize KSP?
    KSPCreate(PETSC_COMM_WORLD,&ksp1);
    KSPSetFromOptions(ksp1);
    KSPAppendOptionsPrefix(ksp2,"s1_");
    KSPCreate(PETSC_COMM_WORLD,&ksp2);
    KSPAppendOptionsPrefix(ksp2,"s2_");
    
    Define three ksp in __mfem_pc_shell_ctx
    Then destroy them in __mfem_pc_shell_destroy

what would be a good initial guess? Use zero for now.



5/21
switch solver to true dofs for ImplicitSolve

5/20
swich J back to auxilary variable (keep the original implement in exMHD for debugging purpose)


5/17
Issues:
x it has to assemble nv and nb each time step, not sure how to do it implicitly (am I doing matrix-free for those two? Probably not)

implemented a new approach based on LinearForm, but it is slower

todo for imMHDp
1. treat J as a true auxiliary variable
2. implement a new class for implicit scheme and reduced operator


5/15
Q1. what happens to GetGradient when factory is used? It is called through __mfem_snes_jacobian
2. Where is Mult implemented now? It is implemented through J->Mult from GetGradient
3. reducedOperator->Mult is something else (not the same)

see submatrix in (a good example on multiphysics coupling)
https://www.mcs.anl.gov/petsc/petsc-current/src/snes/examples/tutorials/ex28.c.html

Question: how could I access the block vec from y??
MatGetLocalSubMatrix
VecGetLocalVector
Those will determine the local matrix and local vectors

Define the linearform operator:
the linear integral order is probably 3k/2
define a special coefficient by myself
then define a linearform and assemble


5/11
-snes_mf_operator is working 


5/6
mpirun -n 1 exAMRMHDp -m xperiodic.mesh -rs 3 -o 4 -tf 8 -vs 10 -dt .0001 -i 3 -amrl 3 -ltol 1e-3
mpirun -n 4 exAMRMHDp -m xperiodic.mesh -rs 3 -o 4 -tf 8 -vs 10 -dt .0001 -i 3 -amrl 3 -ltol 1e-3

derefine in parallel amr working:
mpirun -n 16 exAMRMHDp -m xperiodicR1.mesh -rs 0 -rp 2 -o 4 -tf 8 -vs 200 -dt .0001 -i 3 -amrl 3 -ltol 2e-3 -derefine

5/4
on nersc debug mode has issue (fixed)
make config MFEM_USE_MPI=YES MFEM_DEBUG=YES MPICXX=CC
standard is ok:
make config MFEM_USE_MPI=YES MFEM_DEBUG=NO MPICXX=CC

serial config:
make config MFEM_USE_MPI=NO MFEM_DEBUG=NO CXX=CC

5/3
1. Try derefine
2. Prallel AMR

Consider:
JFNK
Poisson Bracket
SMG
Mixed FEM


4/30
Test it on nersc
srun -n 32 ../exMHDp -m ../xperiodic.mesh -rs 6 -rp 0 -o 3 -tf 1 -vs 2500 -dt .0002 -i 3 -no-vis -visit

profiling this case:
mpirun -n 48 ../exMHDp -m ../xperiodic.mesh -rs 6 -rp 1 -o 3 -tf .1 -vs 1000 -dt .00005 -i 3

4/29
After fixing a bug, parallel code works well:
mpirun -n 96 ../exMHDp -m ../xperiodic.mesh -rs 6 -rp 0 -o 3 -tf 8 -vs 2500 -dt .0002 -i 3 -no-vis -visit

4/27
parallel code appears working

for icase=3
need to reduce rel_tol to invert the mass matrix
it is better to use different rel_tol for mass and stiffness matrices

BoomerAMG is also working
mpirun -n 8 exMHDp -m xperiodic.mesh -rs 3 -rp 2 -o 3 -tf 1 -vs 50 -dt .001 -i 3

4/26
the paralle code sometimes work but sometimes give me unstable results

ok:
mpirun -n 2 exMHDp -rs 2 -tf 10 -vs 50 -dt .004 -i 2
mpirun -n 4 exMHDp -rs 3 -o 3 -tf 3 -vs 100 -dt .001 -no-vis

there is probably a bug in the saving function (got some numbers of 1e-312)

I cannot make vis working. Maybe there is a bug in the code?

this works:
mpirun -n 4 exMHDp -rs 2 -rp 2 -tf 10 -vs 50 -dt .0005 -i 2 -visit -no-vis

this fails on mac (there is a bug somewhere) but works on hpcc:
mpirun -n 4 exMHDp -rs 4 -rp 0 -tf .001 -vs 100 -dt .001 -i 2 -no-vis

this also fails:
mpirun -n 4 exMHDp -rs 2 -o 2 -tf .002 -vs 100 -dt .002

this sometimes works and sometimes not (works on intel14 but not intel16):
mpirun -n 4 exMHDp -rs 2 -o 2 -tf 1.5 -vs 100 -dt .002

4/25
there is a strange bug associate with M->Mult (not consistent in serial and parallel)

4/23
try to benchmark against Pixie2d; the ic example give different resutls with the same initial condition
o found the missing source term in the ic case


4/12
parallel version:
mesh -> ParMesh
FiniteElementSpace -> ParFiniteElementSpace
GridFunction -> ParGridFunction
save solution needs to be changed and visual as well

myCoefficient.hpp? probably could stay sound
Change some preconditioner in ResistiveMHDOperator.hpp


4/10
try:
Start with an AMR mesh
Refine every 1000 time steps
./exAMRMHD -m xperiodicR3.mesh -r 0 -o 4 -tf 8 -vs 10 -dt .0001 -i 3 -amrl 3 -ltol 1e-3

in the inner loop of refinement, I could potential do an iteration!

4/9
grid space 5e-3 at x point for island
try back solve Psi through J! not working!
Evolve current? maybe not there yet

do some iterations first and obtain a better initial grid!!

>>>Do AMR grid generation first
./generateAMR -m xperiodicR3.mesh -r 0 -o 4 -i 3 -amrl 3 -ltol 1e-10

4/8
>>>derefiner is a major source to introduce osscillations
Demo: 129->130 (see the oscillations in omega!):
Number of unknowns: 52336
Number of elements in mesh: 3024
step 130, t = 0.013
Derefined mesh...
True V size = 46496
Problem size = 51952
Number of unknowns: 51952
Number of elements in mesh: 3000

>>>K operator plus an interpolation (and grad J) is the secondary issue

try
./exAMRMHD -m xperiodicR3.mesh -r 0 -o 4 -tf 8 -vs 1000 -dt .0001 -i 3 -amrl 3 -ltol 5e-3 -visit

4/6
this is sort of working
 ./exAMRMHD -m xperiodic.mesh -r 3 -o 4 -tf 10 -vs 50 -dt .0001 -i 3 -ltol 1e-3

after adding refinement level, this works better
./exAMRMHD -m xperiodic.mesh -r 3 -o 4 -tf .02 -vs 50 -dt .0001 -i 3 -amrl 3 -ltol 1e-3


4/5
AMR starts to work after I switch vector size total dofs

4/3
there are some issues for AMR:
o KB add more degrees freedom for nonconforming AMR (see by trun off refiner.SetTotalErrorFraction(0.0))
  I can probably fix it with Mark's suggestion.
x When it did uniform refinement, KB is okay but Current is very osscillatory (P4 is better but I am surprised by the results)
  see (by turn on refiner.SetTotalErrorFraction(0.0)):
  ./exAMRMHD -m xperiodic.mesh -r 3 -o 3 -tf 10 -vs 50 -dt .002 -i 3 -ltol 1e-6
x I do not know how to add the AMR levels (probably control through nc_limit)
x Derefine is not tested yet

there are one issue for nonlinear:
x I have to assemble matrix whenever the nonlinear operator called

o it is not doing uniform refinement any more

We may need to have two requests:
o fix KB for nonconforming AMR
o install the nonlinear operator

4/2
this looks okay, although it does some uniform refinement:
./exAMRMHD -r 3 -o 3 -tf 10 -vs 50 -dt .002 -i 2 -ltol 1e-6

island test:
./exAMRMHD -m xperiodic.mesh -r 3 -o 3 -tf 10 -vs 50 -dt .002 -i 3 -ltol 1e-6

amr test is strange:
current is oscillatory
amr can only do uniform refinement

4/1
there is a strange bug: I have to store coefficients for diffusion operator (deleting them to quick leads to issue in assembling later)
not able to get block vector working with update

3/30
working on amr: 
o define a new estimator: error=erro1+ratio*error2
  Note that since the errors are computed to the total error, it is fine as long as ratio>0.
  By default, ratio = 1.

Ideally, I should define a new ThresholdRefiner that refine the union of two refining region
of two solution. This is not hard. But I am not sure how to do similarly for Derefiner.


3/25
fix a bug from boundary condition when compute J=nable Psi (it did not consider boundary condition)

3/23
clean up the explicit code; move on to test AMR (explicit)

Should I remove nodes in the mesh (nodes only for ALE type methods??)?

AMR with explicit time stepping is not very clear
How to do subcyling?


good example on amr:
 ./ex6 -m ../data/fichera.mesh -o 2

3/22 
After fix the rhs boundary bug, OK for wave case
tearing mode looks ok so far

exMHD -r 4 -tf 250 -vs 200 -dt .001 -i 2

3/21
o add visit

for o = 2
the solutions appear to be first order (from omega and phi); it looks okay
there is a boundary layer for omega

for o3 r4, the errors from boundary become very large; this is strange

Test:
exMHD -m ../xperiodic-square.mesh -r 2 -o 2 -tf 3 -vs 50 -dt .001 -visit
exMHD -m ../xperiodic-square.mesh -r 3 -o 2 -tf 3 -vs 100 -dt .0005 -visit

find a potential bug in the boundary: how could I impose dirichlet boundary in the right hand side?

redo those examples:
../exMHD -m ../xperiodic-square.mesh -r 2 -o 3 -tf 3 -vs 100 -dt .001 -visit
../exMHD -m ../xperiodic-square.mesh -r 3 -o 3 -tf 3 -vs 200 -dt .0005 -visit
../exMHD -m ../xperiodic-square.mesh -r 4 -o 3 -tf 3 -vs 400 -dt .00025 -visit


3/20
o fix a ploting issue for order>2
o fix the boundary orientation, I believe the normal has to be outward
o fix the time stepping
o add visual



3/19
the boundary seems working by using but I do not know why it works...

   const int dim=2;
   VectorArrayCoefficient force(dim);
   for (int i = 0; i < dim-1; i++)
   {
      force.Set(i, new ConstantCoefficient(0.0));
   }
   {
      Vector pull_force(fespace.GetMesh()->bdr_attributes.Max());
      pull_force = -1.0;
      force.Set(dim-1, new PWConstCoefficient(pull_force));
   }

   b.AddDomainIntegrator(new VectorBoundaryLFIntegrator(force));
   b.Assemble();

   K->FormLinearSystem(ess_tdof_list, psi, b, A, X, B);

   so I will subtract B from z.

>>>using this fix in the full code gives me errors associated with memory, I think I did something very crazy.

Tzanio sent me the working code. What is the difference between AddBdrFaceIntegrator and AddBoundaryIntegrator?


3/13
the explicit code is working, however (they are probably all related to the boundary)
the current is still not right,
the boundary condition is not right

note the boundary condition for current should be 0, which is wrong now
If I turned off background B/Phi, then the current solver is right!
Reason:
M*J=K*Psi; however J=0 along the boundary; K*Psi=Psi along the boundary which is inconsistent, I need subtract those off

Luis: if we can figure out how to do it matrix-free in the explicit scheme, we should be good for the implicit scheme. The only matrix that needs to assemble is the mass and stiffness matrix (once in the how solve)

Q:
what solvers for mass and stiffness matrices I am using?

2019/3/6
the stucture of explicit scheme seems clear
the linear parts seem ok
the nonlinear term requires some work:
1. for explicit scheme, it is okay: I need to update matrix inside time step (with proper assembling, I am still not sure about how to do it). both PDSolver and exMHD need some work.
2. for nonlinear problem, how could link nonlinear term properly??

2019/3/3
move back to MFEM

>>>plot solutions at each time step (see ex16)
first open a glvis as a server in another terminal
then run the program; it should plot results auto

11/1
some useful info for developer:
https://github.com/mfem/mfem/blob/master/CONTRIBUTING.md#code-overview

10/30/2018
I believe I could use FE solver by providing the right hand side in two updates

Then I could built right side separatedly
